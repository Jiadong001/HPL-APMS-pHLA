{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tape import TAPETokenizer, ProteinBertConfig\n",
    "from model_ft import meanTAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case\n",
    "given_HLA = \"HLA-B*42:01\"\n",
    "given_peptide = \"RPGGKKKYK\"     # or a list of peptides, e.g. [\"RPGGKKKYK\", \"RTSKAALER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Model preparing\n",
      "Load model from HPL-Pan/cat_mean_2mlp/main_finetune_plm_tape_B32_LR3e-05_seq_clip_fold4_ep51_221104.pkl\n",
      ">>> Model preparing done\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and use_cuda) else \"cpu\")\n",
    "model_path = \"/data/lujd/neoag_model/main_task/\"\n",
    "model1_filename = \"HPL-Pan/cat_mean_2mlp/main_finetune_plm_tape_B32_LR3e-05_seq_clip_fold4_ep51_221104.pkl\"\n",
    "model_names = [model1_filename]         # add more filenames for HPL-Allele\n",
    "\n",
    "print(\">>> Model preparing\")\n",
    "tokenizer = TAPETokenizer(vocab='iupac')\n",
    "tape_config = ProteinBertConfig.from_pretrained('bert-base')\n",
    "models = []\n",
    "for model_name in model_names:\n",
    "    model = meanTAPE(tape_config, \"2mlp\").to(device)\n",
    "    model.load_state_dict(\n",
    "        torch.load(os.path.join(model_path, model_name), map_location=device), \n",
    "        strict = True)\n",
    "    model = model.eval()\n",
    "    models.append(model)\n",
    "    print(f\"Load model from {model_name}\")\n",
    "\n",
    "print(\">>> Model preparing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Input preparing\n",
      "Load HLA allele2sequence dict\n",
      "Convert sequence to tokens\n",
      ">>> Input preparing done\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs\n",
    "print(\">>> Input preparing\")\n",
    "\n",
    "print(\"Load HLA allele2sequence dict\")\n",
    "data_path = \"/data/lujd/neoag_data/\"\n",
    "hla_seq_dict = pd.read_csv(\n",
    "    os.path.join(data_path, \"main_task/HLA_sequence_dict_ABCEG.csv\"),\n",
    "    index_col=0\n",
    "    ).set_index([\"HLA_name\"])[\"clip\"].to_dict()\n",
    "HLA_seq = hla_seq_dict[given_HLA]\n",
    "\n",
    "def seq2token(tokenizer, hla_seq, pep_seq, hla_max_len=182, pep_max_len=15):\n",
    "    pep_tokens, hla_pep_tokens = [], []\n",
    "    \n",
    "    hla_seq = hla_seq.ljust(hla_max_len, 'X')\n",
    "    hla_token = tokenizer.encode(hla_seq)\n",
    "\n",
    "    if type(pep_seq) == str:\n",
    "        pep_seq = [pep_seq]\n",
    "\n",
    "    for seq in pep_seq:\n",
    "        seq = seq.ljust(pep_max_len, 'X')\n",
    "        pep_tokens.append(tokenizer.encode(seq))        # [array]\n",
    "\n",
    "        phla_seq = hla_seq + seq\n",
    "        hla_pep_tokens.append(tokenizer.encode(phla_seq))\n",
    "    \n",
    "    return np.array(hla_token), np.array(pep_tokens), np.array(hla_pep_tokens)\n",
    "\n",
    "print(\"Convert sequence to tokens\")\n",
    "_, _, hla_pep_tokens = seq2token(tokenizer, HLA_seq, given_peptide)\n",
    "hla_pep_inputs = torch.LongTensor(hla_pep_tokens).to(device)\n",
    "\n",
    "print(\">>> Input preparing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLA: HLA-B*42:01, peptide: RPGGKKKYK | binding porbability: 0.0008\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "for ind, model in enumerate(models):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(hla_pep_inputs)\n",
    "        score = output[:, 1] - output[:, 0]\n",
    "        score =score.cpu().detach().numpy()         # 1-D\n",
    "        if ind == 0:\n",
    "            score_ensemble = score\n",
    "        else:\n",
    "            score_ensemble = score_ensemble + score\n",
    "score_ensemble = score_ensemble / len(models)\n",
    "prob = 1 / (1 + np.exp(-score_ensemble))            # sigmod\n",
    "\n",
    "if type(given_peptide) == str:\n",
    "    given_peptide = [given_peptide]\n",
    "\n",
    "for i, pep in enumerate(given_peptide):\n",
    "    print(\"HLA: {}, peptide: {} | binding porbability: {:.4f}\".format(given_HLA, pep, prob[i].item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
